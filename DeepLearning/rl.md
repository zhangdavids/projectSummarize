

RL与其他机器学习算法不同的地方在于：

其中没有监督者，只有一个reward信号；反馈是延迟的，不是立即生成的；

时间在RL中具有重要的意义；agent的行为会影响之后一系列的data。 


RL采用的是边获得样例边学习的方式，在获得样例之后更新自己的模型，利用当前的模型来指导下一步的行动，下一步的行动获得reward之后再更新模型，

不断迭代重复直到模型收敛。在这个过程中，非常重要的一点在于“在已有当前模型的情况下，如果选择下一步的行动才对完善当前的模型最有利”，

这就涉及到了RL中的两个非常重要的概念：探索（exploration）和开发（exploitation），

exploration是指选择之前未执行过的actions，从而探索更多的可能性；exploitation是指选择已执行过的actions，从而对已知的actions的模型进行完善。

RL非常像是“trial-and-error learning”，在尝试和试验中发现好的policy。
